---
title: "HTTPS usage of scholarly publishers websites"
author: "Chris Hartgerink"
---

Scholarly publishers play a major role in the dissemination of scholarly information. As a society, we need to be able to rely on these publishers to provide that information securely, accurately, and with content integrity. We also want to be ensured that the (personal) information we provide is secure (e.g., passwords). In this way, scholarly publishers have a responsibility towards the scholarly community.

Anecdotally, I was surprised how often scholarly publisher's pages are HTTP only. Implementing HTTPS has become much easier with initiatives such as [Let's Encrypt](https://letsencrypt.org/) and [Certbot](https://certbot.eff.org/) (but I recognize legacy systems can make it more difficult). As a scholar, I am concerned with content integrity. This is essential when conducting systematic reviews, meta-analyses, or simply reading research and planning new studies. As a person, I am concerned about, for example, my login credentials and those of my colleagues. [Given that passwords are often reused](http://www.covert.io/research-papers/security/The%20Tangled%20Web%20of%20Password%20Reuse.pdf), HTTP based publisher pages are a threat to the credentials of people visiting the pages of scholarly publishers beyond just their own pages.

In order to hold the disseminators of scholarly information accountable, we need to be able to recognize whether this is a widespread issue and where improvements can be made. For example, one of the most acclaimed journals (Science Magazine) apparently considers HTTP good enough and does not make a statement about why it has not upgraded yet. But there are many more publishers that also have the same responsibility towards their user.

By checking the websites of the publishers indexed by [CrossRef](https://crossref.org), the main metadata store for scholarly publications, we can get a sense of the scope of the problem. Today, there are approximately [10,000 members](https://api.crossref.org/members?rows=0) in CrossRef, of which ~7500 are actively publishing (taking having published in 2017 as being active). 

With the project I am trying to canvas the scholarly publisher's landscape for HTTPS --- it is also a precursor to actually trying to improve the situation. By being able to identify those publishers who publish the largest body of work in an unsecure way, we can start a dialogue with them to improve the situation. Previously, I have had constructive dialogue with [Collabra](https://collabra.org), who upgraded their webpage to default to HTTPS some time after contacting them. If publishers take a negligent or waiving position, this also needs to be known because it belittles the security of the users and their role in accurate content presentation. In the long run, it will hurt the publishers too: [Chrome is starting to label pages as not secure if they use HTTP very soon](https://security.googleblog.com/2018/02/a-secure-web-is-here-to-stay.html). Given that users have no choice if the materials cannot be reshared because the articles are under copyright, publishers have an even larger responsibility to the extended scholarly community.

<!-- scp -i ~/keys/nanopub-ami-key.pem admin@ec2-18-195-231-68.eu-central-1.compute.amazonaws.com:/home/admin/https-checker/db/https-summary.csv /home/chjh/writing/20180419open-source/data.csv -->

<!-- With [increasing centralization of publishing](https://doi.org/10.1371/journal.pone.0127502). -->

```{r echo = FALSE}
dat <- read.csv('20180419open-source/data.csv', stringsAsFactors = FALSE)
fls <- sum(dat$force.https == 'false')
tr <- sum(dat$force.https == 'true')
nll <- sum(dat$force.https == 'null')
prcnt <- round(tr / (tr + fls) * 100, 0)
txt <- ifelse(prcnt >= 10 | prcnt <= 44, 'similar', ifelse(
  prcnt < 10, 'worse', 'better'))
```

The project [`https-checker`](https://github.com/libscie/https-checker) just completed the initial data collection phase. By using [`pshtt`](https://github.com/dhs-ncats/pshtt), an open-source HTTPS testing tool, and a set of calls to the [CrossRef API](https://github.com/CrossRef/rest-api-doc) it was relatively easy to [script an initial canvas](https://github.com/chartgerink/https-checker/blob/master/scripts/https-checker.sh) of the publisher's security practices. 

| Active, default HTTPS | Active, not default HTTPS | Inactive |
| --------------------- | ------------------------- | -------- |
| `r tr` | `r fls` | `r nll` |

At first glance, `r prcnt`% of all `r tr + fls` active publishers default to HTTPS. In general, estimates of Websites that default to HTTPS range between [10-44%](https://www.usenix.org/system/files/conference/usenixsecurity17/sec17-felt.pdf). In other words, scholarly publishers seem to do `r txt` at securing their webpages when compared to the general estimates for webpages. I consider that this does not waive them of a responsibility to improve the situation.


```{r echo = FALSE}
logit2prob <- function(logit){
  odds <- exp(logit)
  prob <- odds / (1 + odds)
  return(prob)
}

dat_glm <- dat[dat$force.https != 'null', ]
dat_glm$force.https[dat_glm$force.https == 'true'] <- 1
dat_glm$force.https[dat_glm$force.https == 'false'] <- 0
dat_glm$force.https <- as.numeric(dat_glm$force.https)
dat_glm$pubs.since.2017 <- as.numeric(dat_glm$pubs.since.2017)
dat_glm$pubs.since.2017.centred <- dat_glm$pubs.since.2017 - mean(dat_glm$pubs.since.2017)

model <- glm(force.https ~ 1 + log(pubs.since.2017),
 data = dat_glm,
 family = binomial(link = 'logit'))

pred.avg <- round(logit2prob(model$coefficients[1] + 
  model$coefficients[2] * 
  log(mean(dat_glm$pubs.since.2017, na.rm = TRUE))) * 100, 0)

pred.med <- round(logit2prob(model$coefficients[1] + 
  model$coefficients[2] * 
  log(median(dat_glm$pubs.since.2017, na.rm = TRUE))) * 100, 0)

small <- 100
pred.small <- round(logit2prob(model$coefficients[1] + 
  model$coefficients[2] * 
  log(small)) * 100, 0)

big <- 1000
pred.big <- round(logit2prob(model$coefficients[1] + 
  model$coefficients[2] * 
  log(big)) * 100, 0)

min_pubs <- min(dat_glm$pubs.since.2017, na.rm = TRUE)
max_pubs <- max(dat_glm$pubs.since.2017, na.rm = TRUE)

options(scipen=0, digits=9)
```

<!-- Predicting the defaulting of https using -->
Running a basic logistic regression to try and predict whether publisher's default their pages to HTTPS shows that big publishers are `r ifelse(pred.big > pred.small, 'more', 'less')` likely. More specifically, a publisher with only `r small` publications since 2017 is estimated to have a `r pred.small`% chance of using HTTPS by default, whereas a publisher with `r big` publications since 2017 is estimated to have a `r pred.big`% chance of using HTTPS by default. Given the average amount of publications, the estimated probability of a publisher using HTTPS by default is `r pred.avg`% (median: `r pred.med`%). We see publishers with publications ranging from `r min_pubs` through 1104607.

Next for this project is opening up the dialogue with some of the largest publishers that do not provide HTTPS by default. These conversations will be tracked as well, in order to get a sense of how active and willing they are in improving the security of the users and content they serve.

The HTTPS scan can however go much deeper than just checking whether the page defaults to HTTPS. Other practices can help improve security even more. For example, the preloading of a HTTP Strict Transport Security (HSTS) header can help mitigate man-in-the-middle attacks. By using in-depth assessments of the webpages, improvements for websites that already default to HTTPS can be identified, improving their security even more.

An increased uptake of more secure practices in content transfer on webpages is key to a secure Web and truthful information, which ultimately affects the users that rely on the information transferred over the Internet. Given that misinformation is spreading, it seems like this is low-hanging fruit for 2018.

<!-- 
* visual browsing interface
* more in depth HTTPS scores
* relations between publisher characteristics and security
  * logistic regression
* Dat angle
https://github.com/beakerbrowser/beaker/wiki/Dat-DNS-TXT-records-with-optional-DNS-over-HTTPS -->


<!-- rmarkdown::render('20180419open-source.Rmd', rmarkdown::html_document()) -->